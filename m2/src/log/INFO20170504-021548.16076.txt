Log file created at: 2017/05/04 02:15:48
Running on machine: CSZ220
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0504 02:15:48.860347 15008 caffe.cpp:218] Using GPUs 0
I0504 02:15:49.024376 15008 caffe.cpp:223] GPU 0: GeForce GTX 1070
I0504 02:15:49.306300 15008 solver.cpp:48] Initializing solver from parameters: 
test_iter: 1000
test_interval: 100
base_lr: 0.001
display: 50
max_iter: 40000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 2500
snapshot: 50
snapshot_prefix: "../caffe_model/snapshot/model_transfer"
solver_mode: GPU
device_id: 0
net: "../caffe_model/train_val.prototxt"
train_state {
  level: 0
  stage: ""
}
I0504 02:15:49.307801 15008 solver.cpp:91] Creating training net from net file: ../caffe_model/train_val.prototxt
I0504 02:15:49.308301 15008 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0504 02:15:49.308301 15008 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0504 02:15:49.308301 15008 net.cpp:58] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_file: "../data/mean.binaryproto"
  }
  data_param {
    source: "../data/train_lmdb"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8-cats-dogs"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8-cats-dogs"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8-cats-dogs"
  bottom: "label"
  top: "loss"
}
I0504 02:15:49.320801 15008 layer_factory.hpp:77] Creating layer data
I0504 02:15:49.320801 15008 net.cpp:100] Creating Layer data
I0504 02:15:49.321301 15008 net.cpp:408] data -> data
I0504 02:15:49.321301 15008 net.cpp:408] data -> label
I0504 02:15:49.321301 15008 data_transformer.cpp:25] Loading mean file from: ../data/mean.binaryproto
I0504 02:15:49.322800  2628 db_lmdb.cpp:40] Opened lmdb ../data/train_lmdb
I0504 02:15:49.380800 15008 data_layer.cpp:41] output data size: 256,3,227,227
I0504 02:15:49.570300 15008 net.cpp:150] Setting up data
I0504 02:15:49.571300 15008 net.cpp:157] Top shape: 256 3 227 227 (39574272)
I0504 02:15:49.571300 15008 net.cpp:157] Top shape: 256 (256)
I0504 02:15:49.571801 15008 net.cpp:165] Memory required for data: 158298112
I0504 02:15:49.571801 15008 layer_factory.hpp:77] Creating layer conv1
I0504 02:15:49.571801 15008 net.cpp:100] Creating Layer conv1
I0504 02:15:49.571801 15008 net.cpp:434] conv1 <- data
I0504 02:15:49.572301 15008 net.cpp:408] conv1 -> conv1
I0504 02:15:49.931299 15008 net.cpp:150] Setting up conv1
I0504 02:15:49.931802 15008 net.cpp:157] Top shape: 256 96 55 55 (74342400)
I0504 02:15:49.931802 15008 net.cpp:165] Memory required for data: 455667712
I0504 02:15:49.931802 15008 layer_factory.hpp:77] Creating layer relu1
I0504 02:15:49.931802 15008 net.cpp:100] Creating Layer relu1
I0504 02:15:49.931802 15008 net.cpp:434] relu1 <- conv1
I0504 02:15:49.931802 15008 net.cpp:395] relu1 -> conv1 (in-place)
I0504 02:15:49.932301 15008 net.cpp:150] Setting up relu1
I0504 02:15:49.932301 15008 net.cpp:157] Top shape: 256 96 55 55 (74342400)
I0504 02:15:49.932301 15008 net.cpp:165] Memory required for data: 753037312
I0504 02:15:49.932301 15008 layer_factory.hpp:77] Creating layer pool1
I0504 02:15:49.932801 15008 net.cpp:100] Creating Layer pool1
I0504 02:15:49.932801 15008 net.cpp:434] pool1 <- conv1
I0504 02:15:49.932801 15008 net.cpp:408] pool1 -> pool1
I0504 02:15:49.932801 15008 net.cpp:150] Setting up pool1
I0504 02:15:49.932801 15008 net.cpp:157] Top shape: 256 96 27 27 (17915904)
I0504 02:15:49.932801 15008 net.cpp:165] Memory required for data: 824700928
I0504 02:15:49.932801 15008 layer_factory.hpp:77] Creating layer norm1
I0504 02:15:49.933300 15008 net.cpp:100] Creating Layer norm1
I0504 02:15:49.933300 15008 net.cpp:434] norm1 <- pool1
I0504 02:15:49.933300 15008 net.cpp:408] norm1 -> norm1
I0504 02:15:49.933801 15008 net.cpp:150] Setting up norm1
I0504 02:15:49.933801 15008 net.cpp:157] Top shape: 256 96 27 27 (17915904)
I0504 02:15:49.933801 15008 net.cpp:165] Memory required for data: 896364544
I0504 02:15:49.933801 15008 layer_factory.hpp:77] Creating layer conv2
I0504 02:15:49.934300 15008 net.cpp:100] Creating Layer conv2
I0504 02:15:49.934300 15008 net.cpp:434] conv2 <- norm1
I0504 02:15:49.934300 15008 net.cpp:408] conv2 -> conv2
I0504 02:15:49.938801 15008 net.cpp:150] Setting up conv2
I0504 02:15:49.939301 15008 net.cpp:157] Top shape: 256 256 27 27 (47775744)
I0504 02:15:49.939800 15008 net.cpp:165] Memory required for data: 1087467520
I0504 02:15:49.939800 15008 layer_factory.hpp:77] Creating layer relu2
I0504 02:15:49.939800 15008 net.cpp:100] Creating Layer relu2
I0504 02:15:49.939800 15008 net.cpp:434] relu2 <- conv2
I0504 02:15:49.939800 15008 net.cpp:395] relu2 -> conv2 (in-place)
I0504 02:15:49.940801 15008 net.cpp:150] Setting up relu2
I0504 02:15:49.941301 15008 net.cpp:157] Top shape: 256 256 27 27 (47775744)
I0504 02:15:49.941301 15008 net.cpp:165] Memory required for data: 1278570496
I0504 02:15:49.941301 15008 layer_factory.hpp:77] Creating layer pool2
I0504 02:15:49.941301 15008 net.cpp:100] Creating Layer pool2
I0504 02:15:49.941800 15008 net.cpp:434] pool2 <- conv2
I0504 02:15:49.941800 15008 net.cpp:408] pool2 -> pool2
I0504 02:15:49.941800 15008 net.cpp:150] Setting up pool2
I0504 02:15:49.941800 15008 net.cpp:157] Top shape: 256 256 13 13 (11075584)
I0504 02:15:49.941800 15008 net.cpp:165] Memory required for data: 1322872832
I0504 02:15:49.941800 15008 layer_factory.hpp:77] Creating layer norm2
I0504 02:15:49.942301 15008 net.cpp:100] Creating Layer norm2
I0504 02:15:49.942301 15008 net.cpp:434] norm2 <- pool2
I0504 02:15:49.942301 15008 net.cpp:408] norm2 -> norm2
I0504 02:15:49.942800 15008 net.cpp:150] Setting up norm2
I0504 02:15:49.942800 15008 net.cpp:157] Top shape: 256 256 13 13 (11075584)
I0504 02:15:49.942800 15008 net.cpp:165] Memory required for data: 1367175168
I0504 02:15:49.942800 15008 layer_factory.hpp:77] Creating layer conv3
I0504 02:15:49.942800 15008 net.cpp:100] Creating Layer conv3
I0504 02:15:49.943301 15008 net.cpp:434] conv3 <- norm2
I0504 02:15:49.943301 15008 net.cpp:408] conv3 -> conv3
I0504 02:15:49.954300 15008 net.cpp:150] Setting up conv3
I0504 02:15:49.954300 15008 net.cpp:157] Top shape: 256 384 13 13 (16613376)
I0504 02:15:49.954300 15008 net.cpp:165] Memory required for data: 1433628672
I0504 02:15:49.954300 15008 layer_factory.hpp:77] Creating layer relu3
I0504 02:15:49.954800 15008 net.cpp:100] Creating Layer relu3
I0504 02:15:49.954800 15008 net.cpp:434] relu3 <- conv3
I0504 02:15:49.954800 15008 net.cpp:395] relu3 -> conv3 (in-place)
I0504 02:15:49.955302 15008 net.cpp:150] Setting up relu3
I0504 02:15:49.955302 15008 net.cpp:157] Top shape: 256 384 13 13 (16613376)
I0504 02:15:49.955302 15008 net.cpp:165] Memory required for data: 1500082176
I0504 02:15:49.955302 15008 layer_factory.hpp:77] Creating layer conv4
I0504 02:15:49.955302 15008 net.cpp:100] Creating Layer conv4
I0504 02:15:49.955302 15008 net.cpp:434] conv4 <- conv3
I0504 02:15:49.955801 15008 net.cpp:408] conv4 -> conv4
I0504 02:15:49.964802 15008 net.cpp:150] Setting up conv4
I0504 02:15:49.965301 15008 net.cpp:157] Top shape: 256 384 13 13 (16613376)
I0504 02:15:49.965301 15008 net.cpp:165] Memory required for data: 1566535680
I0504 02:15:49.965301 15008 layer_factory.hpp:77] Creating layer relu4
I0504 02:15:49.965301 15008 net.cpp:100] Creating Layer relu4
I0504 02:15:49.965301 15008 net.cpp:434] relu4 <- conv4
I0504 02:15:49.965301 15008 net.cpp:395] relu4 -> conv4 (in-place)
I0504 02:15:49.965801 15008 net.cpp:150] Setting up relu4
I0504 02:15:49.965801 15008 net.cpp:157] Top shape: 256 384 13 13 (16613376)
I0504 02:15:49.965801 15008 net.cpp:165] Memory required for data: 1632989184
I0504 02:15:49.965801 15008 layer_factory.hpp:77] Creating layer conv5
I0504 02:15:49.965801 15008 net.cpp:100] Creating Layer conv5
I0504 02:15:49.966301 15008 net.cpp:434] conv5 <- conv4
I0504 02:15:49.966301 15008 net.cpp:408] conv5 -> conv5
I0504 02:15:49.972800 15008 net.cpp:150] Setting up conv5
I0504 02:15:49.973301 15008 net.cpp:157] Top shape: 256 256 13 13 (11075584)
I0504 02:15:49.973301 15008 net.cpp:165] Memory required for data: 1677291520
I0504 02:15:49.973301 15008 layer_factory.hpp:77] Creating layer relu5
I0504 02:15:49.973301 15008 net.cpp:100] Creating Layer relu5
I0504 02:15:49.973301 15008 net.cpp:434] relu5 <- conv5
I0504 02:15:49.973301 15008 net.cpp:395] relu5 -> conv5 (in-place)
I0504 02:15:49.973800 15008 net.cpp:150] Setting up relu5
I0504 02:15:49.973800 15008 net.cpp:157] Top shape: 256 256 13 13 (11075584)
I0504 02:15:49.973800 15008 net.cpp:165] Memory required for data: 1721593856
I0504 02:15:49.973800 15008 layer_factory.hpp:77] Creating layer pool5
I0504 02:15:49.974301 15008 net.cpp:100] Creating Layer pool5
I0504 02:15:49.974301 15008 net.cpp:434] pool5 <- conv5
I0504 02:15:49.974301 15008 net.cpp:408] pool5 -> pool5
I0504 02:15:49.974301 15008 net.cpp:150] Setting up pool5
I0504 02:15:49.974301 15008 net.cpp:157] Top shape: 256 256 6 6 (2359296)
I0504 02:15:49.974800 15008 net.cpp:165] Memory required for data: 1731031040
I0504 02:15:49.974800 15008 layer_factory.hpp:77] Creating layer fc6
I0504 02:15:49.974800 15008 net.cpp:100] Creating Layer fc6
I0504 02:15:49.974800 15008 net.cpp:434] fc6 <- pool5
I0504 02:15:49.974800 15008 net.cpp:408] fc6 -> fc6
I0504 02:15:50.304805 15008 net.cpp:150] Setting up fc6
I0504 02:15:50.305301 15008 net.cpp:157] Top shape: 256 4096 (1048576)
I0504 02:15:50.305301 15008 net.cpp:165] Memory required for data: 1735225344
I0504 02:15:50.305301 15008 layer_factory.hpp:77] Creating layer relu6
I0504 02:15:50.305301 15008 net.cpp:100] Creating Layer relu6
I0504 02:15:50.305802 15008 net.cpp:434] relu6 <- fc6
I0504 02:15:50.305802 15008 net.cpp:395] relu6 -> fc6 (in-place)
I0504 02:15:50.306303 15008 net.cpp:150] Setting up relu6
I0504 02:15:50.306303 15008 net.cpp:157] Top shape: 256 4096 (1048576)
I0504 02:15:50.306803 15008 net.cpp:165] Memory required for data: 1739419648
I0504 02:15:50.306803 15008 layer_factory.hpp:77] Creating layer drop6
I0504 02:15:50.306803 15008 net.cpp:100] Creating Layer drop6
I0504 02:15:50.306803 15008 net.cpp:434] drop6 <- fc6
I0504 02:15:50.306803 15008 net.cpp:395] drop6 -> fc6 (in-place)
I0504 02:15:50.307301 15008 net.cpp:150] Setting up drop6
I0504 02:15:50.307301 15008 net.cpp:157] Top shape: 256 4096 (1048576)
I0504 02:15:50.307301 15008 net.cpp:165] Memory required for data: 1743613952
I0504 02:15:50.307301 15008 layer_factory.hpp:77] Creating layer fc7
I0504 02:15:50.307301 15008 net.cpp:100] Creating Layer fc7
I0504 02:15:50.307801 15008 net.cpp:434] fc7 <- fc6
I0504 02:15:50.307801 15008 net.cpp:408] fc7 -> fc7
I0504 02:15:50.456300 15008 net.cpp:150] Setting up fc7
I0504 02:15:50.456300 15008 net.cpp:157] Top shape: 256 4096 (1048576)
I0504 02:15:50.456801 15008 net.cpp:165] Memory required for data: 1747808256
I0504 02:15:50.456801 15008 layer_factory.hpp:77] Creating layer relu7
I0504 02:15:50.456801 15008 net.cpp:100] Creating Layer relu7
I0504 02:15:50.456801 15008 net.cpp:434] relu7 <- fc7
I0504 02:15:50.457300 15008 net.cpp:395] relu7 -> fc7 (in-place)
I0504 02:15:50.457300 15008 net.cpp:150] Setting up relu7
I0504 02:15:50.457800 15008 net.cpp:157] Top shape: 256 4096 (1048576)
I0504 02:15:50.457800 15008 net.cpp:165] Memory required for data: 1752002560
I0504 02:15:50.457800 15008 layer_factory.hpp:77] Creating layer drop7
I0504 02:15:50.457800 15008 net.cpp:100] Creating Layer drop7
I0504 02:15:50.457800 15008 net.cpp:434] drop7 <- fc7
I0504 02:15:50.457800 15008 net.cpp:395] drop7 -> fc7 (in-place)
I0504 02:15:50.457800 15008 net.cpp:150] Setting up drop7
I0504 02:15:50.458300 15008 net.cpp:157] Top shape: 256 4096 (1048576)
I0504 02:15:50.458300 15008 net.cpp:165] Memory required for data: 1756196864
I0504 02:15:50.458300 15008 layer_factory.hpp:77] Creating layer fc8-cats-dogs
I0504 02:15:50.458300 15008 net.cpp:100] Creating Layer fc8-cats-dogs
I0504 02:15:50.458300 15008 net.cpp:434] fc8-cats-dogs <- fc7
I0504 02:15:50.458300 15008 net.cpp:408] fc8-cats-dogs -> fc8-cats-dogs
I0504 02:15:50.459301 15008 net.cpp:150] Setting up fc8-cats-dogs
I0504 02:15:50.459301 15008 net.cpp:157] Top shape: 256 2 (512)
I0504 02:15:50.459801 15008 net.cpp:165] Memory required for data: 1756198912
I0504 02:15:50.459801 15008 layer_factory.hpp:77] Creating layer loss
I0504 02:15:50.459801 15008 net.cpp:100] Creating Layer loss
I0504 02:15:50.459801 15008 net.cpp:434] loss <- fc8-cats-dogs
I0504 02:15:50.459801 15008 net.cpp:434] loss <- label
I0504 02:15:50.459801 15008 net.cpp:408] loss -> loss
I0504 02:15:50.460300 15008 layer_factory.hpp:77] Creating layer loss
I0504 02:15:50.460300 15008 net.cpp:150] Setting up loss
I0504 02:15:50.460300 15008 net.cpp:157] Top shape: (1)
I0504 02:15:50.460801 15008 net.cpp:160]     with loss weight 1
I0504 02:15:50.460801 15008 net.cpp:165] Memory required for data: 1756198916
I0504 02:15:50.460801 15008 net.cpp:226] loss needs backward computation.
I0504 02:15:50.460801 15008 net.cpp:226] fc8-cats-dogs needs backward computation.
I0504 02:15:50.460801 15008 net.cpp:226] drop7 needs backward computation.
I0504 02:15:50.460801 15008 net.cpp:226] relu7 needs backward computation.
I0504 02:15:50.460801 15008 net.cpp:226] fc7 needs backward computation.
I0504 02:15:50.461302 15008 net.cpp:226] drop6 needs backward computation.
I0504 02:15:50.461302 15008 net.cpp:226] relu6 needs backward computation.
I0504 02:15:50.461302 15008 net.cpp:226] fc6 needs backward computation.
I0504 02:15:50.461302 15008 net.cpp:226] pool5 needs backward computation.
I0504 02:15:50.461302 15008 net.cpp:226] relu5 needs backward computation.
I0504 02:15:50.461302 15008 net.cpp:226] conv5 needs backward computation.
I0504 02:15:50.461302 15008 net.cpp:226] relu4 needs backward computation.
I0504 02:15:50.461800 15008 net.cpp:226] conv4 needs backward computation.
I0504 02:15:50.461800 15008 net.cpp:226] relu3 needs backward computation.
I0504 02:15:50.461800 15008 net.cpp:226] conv3 needs backward computation.
I0504 02:15:50.461800 15008 net.cpp:226] norm2 needs backward computation.
I0504 02:15:50.461800 15008 net.cpp:226] pool2 needs backward computation.
I0504 02:15:50.461800 15008 net.cpp:226] relu2 needs backward computation.
I0504 02:15:50.461800 15008 net.cpp:226] conv2 needs backward computation.
I0504 02:15:50.462301 15008 net.cpp:226] norm1 needs backward computation.
I0504 02:15:50.462301 15008 net.cpp:226] pool1 needs backward computation.
I0504 02:15:50.462301 15008 net.cpp:226] relu1 needs backward computation.
I0504 02:15:50.462301 15008 net.cpp:226] conv1 needs backward computation.
I0504 02:15:50.462301 15008 net.cpp:228] data does not need backward computation.
I0504 02:15:50.462301 15008 net.cpp:270] This network produces output loss
I0504 02:15:50.462301 15008 net.cpp:283] Network initialization done.
I0504 02:15:50.462800 15008 solver.cpp:181] Creating test net (#0) specified by net file: ../caffe_model/train_val.prototxt
I0504 02:15:50.462800 15008 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0504 02:15:50.463301 15008 net.cpp:58] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_file: "../data/mean.binaryproto"
  }
  data_param {
    source: "../data/validation_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8-cats-dogs"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8-cats-dogs"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8-cats-dogs"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8-cats-dogs"
  bottom: "label"
  top: "loss"
}
I0504 02:15:50.478301 15008 layer_factory.hpp:77] Creating layer data
I0504 02:15:50.478301 15008 net.cpp:100] Creating Layer data
I0504 02:15:50.478801 15008 net.cpp:408] data -> data
I0504 02:15:50.478801 15008 net.cpp:408] data -> label
I0504 02:15:50.478801 15008 data_transformer.cpp:25] Loading mean file from: ../data/mean.binaryproto
I0504 02:15:50.480300 13916 db_lmdb.cpp:40] Opened lmdb ../data/validation_lmdb
I0504 02:15:50.483304 15008 data_layer.cpp:41] output data size: 50,3,227,227
I0504 02:15:50.526300 15008 net.cpp:150] Setting up data
I0504 02:15:50.526800 15008 net.cpp:157] Top shape: 50 3 227 227 (7729350)
I0504 02:15:50.527302 15008 net.cpp:157] Top shape: 50 (50)
I0504 02:15:50.527302 15008 net.cpp:165] Memory required for data: 30917600
I0504 02:15:50.527302 15008 layer_factory.hpp:77] Creating layer label_data_1_split
I0504 02:15:50.527801 15008 net.cpp:100] Creating Layer label_data_1_split
I0504 02:15:50.527801 15008 net.cpp:434] label_data_1_split <- label
I0504 02:15:50.527801 15008 net.cpp:408] label_data_1_split -> label_data_1_split_0
I0504 02:15:50.528301 15008 net.cpp:408] label_data_1_split -> label_data_1_split_1
I0504 02:15:50.530824 15008 net.cpp:150] Setting up label_data_1_split
I0504 02:15:50.530824 15008 net.cpp:157] Top shape: 50 (50)
I0504 02:15:50.531301 15008 net.cpp:157] Top shape: 50 (50)
I0504 02:15:50.531301 15008 net.cpp:165] Memory required for data: 30918000
I0504 02:15:50.531301 15008 layer_factory.hpp:77] Creating layer conv1
I0504 02:15:50.531301 15008 net.cpp:100] Creating Layer conv1
I0504 02:15:50.531301 15008 net.cpp:434] conv1 <- data
I0504 02:15:50.531800 15008 net.cpp:408] conv1 -> conv1
I0504 02:15:50.533324 15008 net.cpp:150] Setting up conv1
I0504 02:15:50.533324 15008 net.cpp:157] Top shape: 50 96 55 55 (14520000)
I0504 02:15:50.533802 15008 net.cpp:165] Memory required for data: 88998000
I0504 02:15:50.533802 15008 layer_factory.hpp:77] Creating layer relu1
I0504 02:15:50.533802 15008 net.cpp:100] Creating Layer relu1
I0504 02:15:50.533802 15008 net.cpp:434] relu1 <- conv1
I0504 02:15:50.533802 15008 net.cpp:395] relu1 -> conv1 (in-place)
I0504 02:15:50.534301 15008 net.cpp:150] Setting up relu1
I0504 02:15:50.534301 15008 net.cpp:157] Top shape: 50 96 55 55 (14520000)
I0504 02:15:50.534301 15008 net.cpp:165] Memory required for data: 147078000
I0504 02:15:50.534301 15008 layer_factory.hpp:77] Creating layer pool1
I0504 02:15:50.534801 15008 net.cpp:100] Creating Layer pool1
I0504 02:15:50.534801 15008 net.cpp:434] pool1 <- conv1
I0504 02:15:50.534801 15008 net.cpp:408] pool1 -> pool1
I0504 02:15:50.534801 15008 net.cpp:150] Setting up pool1
I0504 02:15:50.534801 15008 net.cpp:157] Top shape: 50 96 27 27 (3499200)
I0504 02:15:50.534801 15008 net.cpp:165] Memory required for data: 161074800
I0504 02:15:50.535300 15008 layer_factory.hpp:77] Creating layer norm1
I0504 02:15:50.536301 15008 net.cpp:100] Creating Layer norm1
I0504 02:15:50.536301 15008 net.cpp:434] norm1 <- pool1
I0504 02:15:50.536301 15008 net.cpp:408] norm1 -> norm1
I0504 02:15:50.537302 15008 net.cpp:150] Setting up norm1
I0504 02:15:50.537302 15008 net.cpp:157] Top shape: 50 96 27 27 (3499200)
I0504 02:15:50.537302 15008 net.cpp:165] Memory required for data: 175071600
I0504 02:15:50.537302 15008 layer_factory.hpp:77] Creating layer conv2
I0504 02:15:50.537801 15008 net.cpp:100] Creating Layer conv2
I0504 02:15:50.537801 15008 net.cpp:434] conv2 <- norm1
I0504 02:15:50.537801 15008 net.cpp:408] conv2 -> conv2
I0504 02:15:50.541801 15008 net.cpp:150] Setting up conv2
I0504 02:15:50.541801 15008 net.cpp:157] Top shape: 50 256 27 27 (9331200)
I0504 02:15:50.542302 15008 net.cpp:165] Memory required for data: 212396400
I0504 02:15:50.542302 15008 layer_factory.hpp:77] Creating layer relu2
I0504 02:15:50.542302 15008 net.cpp:100] Creating Layer relu2
I0504 02:15:50.542302 15008 net.cpp:434] relu2 <- conv2
I0504 02:15:50.542803 15008 net.cpp:395] relu2 -> conv2 (in-place)
I0504 02:15:50.543301 15008 net.cpp:150] Setting up relu2
I0504 02:15:50.543301 15008 net.cpp:157] Top shape: 50 256 27 27 (9331200)
I0504 02:15:50.543301 15008 net.cpp:165] Memory required for data: 249721200
I0504 02:15:50.543800 15008 layer_factory.hpp:77] Creating layer pool2
I0504 02:15:50.543800 15008 net.cpp:100] Creating Layer pool2
I0504 02:15:50.543800 15008 net.cpp:434] pool2 <- conv2
I0504 02:15:50.543800 15008 net.cpp:408] pool2 -> pool2
I0504 02:15:50.544302 15008 net.cpp:150] Setting up pool2
I0504 02:15:50.544302 15008 net.cpp:157] Top shape: 50 256 13 13 (2163200)
I0504 02:15:50.544302 15008 net.cpp:165] Memory required for data: 258374000
I0504 02:15:50.544302 15008 layer_factory.hpp:77] Creating layer norm2
I0504 02:15:50.544800 15008 net.cpp:100] Creating Layer norm2
I0504 02:15:50.544800 15008 net.cpp:434] norm2 <- pool2
I0504 02:15:50.544800 15008 net.cpp:408] norm2 -> norm2
I0504 02:15:50.545301 15008 net.cpp:150] Setting up norm2
I0504 02:15:50.546300 15008 net.cpp:157] Top shape: 50 256 13 13 (2163200)
I0504 02:15:50.546300 15008 net.cpp:165] Memory required for data: 267026800
I0504 02:15:50.546300 15008 layer_factory.hpp:77] Creating layer conv3
I0504 02:15:50.546300 15008 net.cpp:100] Creating Layer conv3
I0504 02:15:50.546800 15008 net.cpp:434] conv3 <- norm2
I0504 02:15:50.546800 15008 net.cpp:408] conv3 -> conv3
I0504 02:15:50.557302 15008 net.cpp:150] Setting up conv3
I0504 02:15:50.557302 15008 net.cpp:157] Top shape: 50 384 13 13 (3244800)
I0504 02:15:50.557302 15008 net.cpp:165] Memory required for data: 280006000
I0504 02:15:50.557801 15008 layer_factory.hpp:77] Creating layer relu3
I0504 02:15:50.557801 15008 net.cpp:100] Creating Layer relu3
I0504 02:15:50.557801 15008 net.cpp:434] relu3 <- conv3
I0504 02:15:50.558301 15008 net.cpp:395] relu3 -> conv3 (in-place)
I0504 02:15:50.558301 15008 net.cpp:150] Setting up relu3
I0504 02:15:50.558301 15008 net.cpp:157] Top shape: 50 384 13 13 (3244800)
I0504 02:15:50.558800 15008 net.cpp:165] Memory required for data: 292985200
I0504 02:15:50.558800 15008 layer_factory.hpp:77] Creating layer conv4
I0504 02:15:50.558800 15008 net.cpp:100] Creating Layer conv4
I0504 02:15:50.558800 15008 net.cpp:434] conv4 <- conv3
I0504 02:15:50.559301 15008 net.cpp:408] conv4 -> conv4
I0504 02:15:50.573801 15008 net.cpp:150] Setting up conv4
I0504 02:15:50.574301 15008 net.cpp:157] Top shape: 50 384 13 13 (3244800)
I0504 02:15:50.574301 15008 net.cpp:165] Memory required for data: 305964400
I0504 02:15:50.574301 15008 layer_factory.hpp:77] Creating layer relu4
I0504 02:15:50.574301 15008 net.cpp:100] Creating Layer relu4
I0504 02:15:50.574800 15008 net.cpp:434] relu4 <- conv4
I0504 02:15:50.574800 15008 net.cpp:395] relu4 -> conv4 (in-place)
I0504 02:15:50.575301 15008 net.cpp:150] Setting up relu4
I0504 02:15:50.575301 15008 net.cpp:157] Top shape: 50 384 13 13 (3244800)
I0504 02:15:50.575301 15008 net.cpp:165] Memory required for data: 318943600
I0504 02:15:50.575301 15008 layer_factory.hpp:77] Creating layer conv5
I0504 02:15:50.575301 15008 net.cpp:100] Creating Layer conv5
I0504 02:15:50.575800 15008 net.cpp:434] conv5 <- conv4
I0504 02:15:50.575800 15008 net.cpp:408] conv5 -> conv5
I0504 02:15:50.583802 15008 net.cpp:150] Setting up conv5
I0504 02:15:50.584301 15008 net.cpp:157] Top shape: 50 256 13 13 (2163200)
I0504 02:15:50.584301 15008 net.cpp:165] Memory required for data: 327596400
I0504 02:15:50.584301 15008 layer_factory.hpp:77] Creating layer relu5
I0504 02:15:50.584301 15008 net.cpp:100] Creating Layer relu5
I0504 02:15:50.584803 15008 net.cpp:434] relu5 <- conv5
I0504 02:15:50.584803 15008 net.cpp:395] relu5 -> conv5 (in-place)
I0504 02:15:50.584803 15008 net.cpp:150] Setting up relu5
I0504 02:15:50.585301 15008 net.cpp:157] Top shape: 50 256 13 13 (2163200)
I0504 02:15:50.585301 15008 net.cpp:165] Memory required for data: 336249200
I0504 02:15:50.585301 15008 layer_factory.hpp:77] Creating layer pool5
I0504 02:15:50.585301 15008 net.cpp:100] Creating Layer pool5
I0504 02:15:50.585801 15008 net.cpp:434] pool5 <- conv5
I0504 02:15:50.585801 15008 net.cpp:408] pool5 -> pool5
I0504 02:15:50.585801 15008 net.cpp:150] Setting up pool5
I0504 02:15:50.586302 15008 net.cpp:157] Top shape: 50 256 6 6 (460800)
I0504 02:15:50.586302 15008 net.cpp:165] Memory required for data: 338092400
I0504 02:15:50.586302 15008 layer_factory.hpp:77] Creating layer fc6
I0504 02:15:50.586802 15008 net.cpp:100] Creating Layer fc6
I0504 02:15:50.586802 15008 net.cpp:434] fc6 <- pool5
I0504 02:15:50.586802 15008 net.cpp:408] fc6 -> fc6
I0504 02:15:50.917800 15008 net.cpp:150] Setting up fc6
I0504 02:15:50.918301 15008 net.cpp:157] Top shape: 50 4096 (204800)
I0504 02:15:50.918301 15008 net.cpp:165] Memory required for data: 338911600
I0504 02:15:50.918802 15008 layer_factory.hpp:77] Creating layer relu6
I0504 02:15:50.918802 15008 net.cpp:100] Creating Layer relu6
I0504 02:15:50.918802 15008 net.cpp:434] relu6 <- fc6
I0504 02:15:50.919301 15008 net.cpp:395] relu6 -> fc6 (in-place)
I0504 02:15:50.919301 15008 net.cpp:150] Setting up relu6
I0504 02:15:50.919800 15008 net.cpp:157] Top shape: 50 4096 (204800)
I0504 02:15:50.919800 15008 net.cpp:165] Memory required for data: 339730800
I0504 02:15:50.919800 15008 layer_factory.hpp:77] Creating layer drop6
I0504 02:15:50.919800 15008 net.cpp:100] Creating Layer drop6
I0504 02:15:50.919800 15008 net.cpp:434] drop6 <- fc6
I0504 02:15:50.920301 15008 net.cpp:395] drop6 -> fc6 (in-place)
I0504 02:15:50.920301 15008 net.cpp:150] Setting up drop6
I0504 02:15:50.920802 15008 net.cpp:157] Top shape: 50 4096 (204800)
I0504 02:15:50.920802 15008 net.cpp:165] Memory required for data: 340550000
I0504 02:15:50.920802 15008 layer_factory.hpp:77] Creating layer fc7
I0504 02:15:50.921300 15008 net.cpp:100] Creating Layer fc7
I0504 02:15:50.921300 15008 net.cpp:434] fc7 <- fc6
I0504 02:15:50.921300 15008 net.cpp:408] fc7 -> fc7
I0504 02:15:51.070330 15008 net.cpp:150] Setting up fc7
I0504 02:15:51.070330 15008 net.cpp:157] Top shape: 50 4096 (204800)
I0504 02:15:51.070802 15008 net.cpp:165] Memory required for data: 341369200
I0504 02:15:51.070802 15008 layer_factory.hpp:77] Creating layer relu7
I0504 02:15:51.070802 15008 net.cpp:100] Creating Layer relu7
I0504 02:15:51.071302 15008 net.cpp:434] relu7 <- fc7
I0504 02:15:51.071302 15008 net.cpp:395] relu7 -> fc7 (in-place)
I0504 02:15:51.072302 15008 net.cpp:150] Setting up relu7
I0504 02:15:51.072302 15008 net.cpp:157] Top shape: 50 4096 (204800)
I0504 02:15:51.072302 15008 net.cpp:165] Memory required for data: 342188400
I0504 02:15:51.072302 15008 layer_factory.hpp:77] Creating layer drop7
I0504 02:15:51.072803 15008 net.cpp:100] Creating Layer drop7
I0504 02:15:51.072803 15008 net.cpp:434] drop7 <- fc7
I0504 02:15:51.072803 15008 net.cpp:395] drop7 -> fc7 (in-place)
I0504 02:15:51.073300 15008 net.cpp:150] Setting up drop7
I0504 02:15:51.073354 15008 net.cpp:157] Top shape: 50 4096 (204800)
I0504 02:15:51.073354 15008 net.cpp:165] Memory required for data: 343007600
I0504 02:15:51.073354 15008 layer_factory.hpp:77] Creating layer fc8-cats-dogs
I0504 02:15:51.073354 15008 net.cpp:100] Creating Layer fc8-cats-dogs
I0504 02:15:51.073354 15008 net.cpp:434] fc8-cats-dogs <- fc7
I0504 02:15:51.073858 15008 net.cpp:408] fc8-cats-dogs -> fc8-cats-dogs
I0504 02:15:51.073858 15008 net.cpp:150] Setting up fc8-cats-dogs
I0504 02:15:51.073858 15008 net.cpp:157] Top shape: 50 2 (100)
I0504 02:15:51.073858 15008 net.cpp:165] Memory required for data: 343008000
I0504 02:15:51.073858 15008 layer_factory.hpp:77] Creating layer fc8-cats-dogs_fc8-cats-dogs_0_split
I0504 02:15:51.074357 15008 net.cpp:100] Creating Layer fc8-cats-dogs_fc8-cats-dogs_0_split
I0504 02:15:51.074357 15008 net.cpp:434] fc8-cats-dogs_fc8-cats-dogs_0_split <- fc8-cats-dogs
I0504 02:15:51.074357 15008 net.cpp:408] fc8-cats-dogs_fc8-cats-dogs_0_split -> fc8-cats-dogs_fc8-cats-dogs_0_split_0
I0504 02:15:51.074357 15008 net.cpp:408] fc8-cats-dogs_fc8-cats-dogs_0_split -> fc8-cats-dogs_fc8-cats-dogs_0_split_1
I0504 02:15:51.074357 15008 net.cpp:150] Setting up fc8-cats-dogs_fc8-cats-dogs_0_split
I0504 02:15:51.074856 15008 net.cpp:157] Top shape: 50 2 (100)
I0504 02:15:51.074856 15008 net.cpp:157] Top shape: 50 2 (100)
I0504 02:15:51.074856 15008 net.cpp:165] Memory required for data: 343008800
I0504 02:15:51.074856 15008 layer_factory.hpp:77] Creating layer accuracy
I0504 02:15:51.074856 15008 net.cpp:100] Creating Layer accuracy
I0504 02:15:51.074856 15008 net.cpp:434] accuracy <- fc8-cats-dogs_fc8-cats-dogs_0_split_0
I0504 02:15:51.075356 15008 net.cpp:434] accuracy <- label_data_1_split_0
I0504 02:15:51.075356 15008 net.cpp:408] accuracy -> accuracy
I0504 02:15:51.075356 15008 net.cpp:150] Setting up accuracy
I0504 02:15:51.075356 15008 net.cpp:157] Top shape: (1)
I0504 02:15:51.075356 15008 net.cpp:165] Memory required for data: 343008804
I0504 02:15:51.075356 15008 layer_factory.hpp:77] Creating layer loss
I0504 02:15:51.075356 15008 net.cpp:100] Creating Layer loss
I0504 02:15:51.075857 15008 net.cpp:434] loss <- fc8-cats-dogs_fc8-cats-dogs_0_split_1
I0504 02:15:51.075857 15008 net.cpp:434] loss <- label_data_1_split_1
I0504 02:15:51.075857 15008 net.cpp:408] loss -> loss
I0504 02:15:51.075857 15008 layer_factory.hpp:77] Creating layer loss
I0504 02:15:51.076356 15008 net.cpp:150] Setting up loss
I0504 02:15:51.076356 15008 net.cpp:157] Top shape: (1)
I0504 02:15:51.076356 15008 net.cpp:160]     with loss weight 1
I0504 02:15:51.076356 15008 net.cpp:165] Memory required for data: 343008808
I0504 02:15:51.076356 15008 net.cpp:226] loss needs backward computation.
I0504 02:15:51.076856 15008 net.cpp:228] accuracy does not need backward computation.
I0504 02:15:51.076856 15008 net.cpp:226] fc8-cats-dogs_fc8-cats-dogs_0_split needs backward computation.
I0504 02:15:51.076856 15008 net.cpp:226] fc8-cats-dogs needs backward computation.
I0504 02:15:51.076856 15008 net.cpp:226] drop7 needs backward computation.
I0504 02:15:51.076856 15008 net.cpp:226] relu7 needs backward computation.
I0504 02:15:51.076856 15008 net.cpp:226] fc7 needs backward computation.
I0504 02:15:51.076856 15008 net.cpp:226] drop6 needs backward computation.
I0504 02:15:51.077356 15008 net.cpp:226] relu6 needs backward computation.
I0504 02:15:51.077356 15008 net.cpp:226] fc6 needs backward computation.
I0504 02:15:51.077356 15008 net.cpp:226] pool5 needs backward computation.
I0504 02:15:51.077356 15008 net.cpp:226] relu5 needs backward computation.
I0504 02:15:51.077356 15008 net.cpp:226] conv5 needs backward computation.
I0504 02:15:51.077356 15008 net.cpp:226] relu4 needs backward computation.
I0504 02:15:51.077857 15008 net.cpp:226] conv4 needs backward computation.
I0504 02:15:51.077857 15008 net.cpp:226] relu3 needs backward computation.
I0504 02:15:51.077857 15008 net.cpp:226] conv3 needs backward computation.
I0504 02:15:51.077857 15008 net.cpp:226] norm2 needs backward computation.
I0504 02:15:51.077857 15008 net.cpp:226] pool2 needs backward computation.
I0504 02:15:51.077857 15008 net.cpp:226] relu2 needs backward computation.
I0504 02:15:51.078356 15008 net.cpp:226] conv2 needs backward computation.
I0504 02:15:51.078356 15008 net.cpp:226] norm1 needs backward computation.
I0504 02:15:51.078356 15008 net.cpp:226] pool1 needs backward computation.
I0504 02:15:51.078356 15008 net.cpp:226] relu1 needs backward computation.
I0504 02:15:51.078356 15008 net.cpp:226] conv1 needs backward computation.
I0504 02:15:51.078356 15008 net.cpp:228] label_data_1_split does not need backward computation.
I0504 02:15:51.078356 15008 net.cpp:228] data does not need backward computation.
I0504 02:15:51.078856 15008 net.cpp:270] This network produces output accuracy
I0504 02:15:51.078856 15008 net.cpp:270] This network produces output loss
I0504 02:15:51.078856 15008 net.cpp:283] Network initialization done.
I0504 02:15:51.078856 15008 solver.cpp:60] Solver scaffolding done.
I0504 02:15:51.079859 15008 caffe.cpp:155] Finetuning from ../caffe_model/bvlc_reference_caffenet.caffemodel
I0504 02:15:51.243357 15008 upgrade_proto.cpp:44] Attempting to upgrade input file specified using deprecated transformation parameters: ../caffe_model/bvlc_reference_caffenet.caffemodel
I0504 02:15:51.243856 15008 upgrade_proto.cpp:47] Successfully upgraded file specified using deprecated data transformation parameters.
W0504 02:15:51.243856 15008 upgrade_proto.cpp:49] Note that future Caffe releases will only support transform_param messages for transformation fields.
I0504 02:15:51.245357 15008 upgrade_proto.cpp:53] Attempting to upgrade input file specified using deprecated V1LayerParameter: ../caffe_model/bvlc_reference_caffenet.caffemodel
I0504 02:15:51.457356 15008 upgrade_proto.cpp:61] Successfully upgraded file specified using deprecated V1LayerParameter
I0504 02:15:51.504858 15008 net.cpp:761] Ignoring source layer fc8
I0504 02:15:51.690385 15008 upgrade_proto.cpp:44] Attempting to upgrade input file specified using deprecated transformation parameters: ../caffe_model/bvlc_reference_caffenet.caffemodel
I0504 02:15:51.690857 15008 upgrade_proto.cpp:47] Successfully upgraded file specified using deprecated data transformation parameters.
W0504 02:15:51.691357 15008 upgrade_proto.cpp:49] Note that future Caffe releases will only support transform_param messages for transformation fields.
I0504 02:15:51.691357 15008 upgrade_proto.cpp:53] Attempting to upgrade input file specified using deprecated V1LayerParameter: ../caffe_model/bvlc_reference_caffenet.caffemodel
I0504 02:15:51.900357 15008 upgrade_proto.cpp:61] Successfully upgraded file specified using deprecated V1LayerParameter
I0504 02:15:51.945857 15008 net.cpp:761] Ignoring source layer fc8
I0504 02:15:51.962357 15008 caffe.cpp:252] Starting Optimization
I0504 02:15:51.962357 15008 solver.cpp:303] Solving CaffeNet
I0504 02:15:51.962858 15008 solver.cpp:304] Learning Rate Policy: step
I0504 02:15:52.009340 15008 solver.cpp:361] Iteration 0, Testing net (#0)
I0504 02:15:55.757802 15008 blocking_queue.cpp:50] Data layer prefetch queue empty
I0504 02:16:12.364111 15008 solver.cpp:428]     Test net output #0: accuracy = 0.34436
I0504 02:16:12.365110 15008 solver.cpp:428]     Test net output #1: loss = 1.01532 (* 1 = 1.01532 loss)
I0504 02:16:12.554605 15008 solver.cpp:234] Iteration 0, loss = 1.21422
I0504 02:16:12.555099 15008 solver.cpp:250]     Train net output #0: loss = 1.21422 (* 1 = 1.21422 loss)
I0504 02:16:12.701099 15008 solver.cpp:264] layer blob norm:0.000000 0.000000 0.000000 0.000000 0.000000 0.000001 0.000006 0.002220 
I0504 02:16:12.703599 15008 solver.cpp:272] weight blob norm:0.011486 0.005885 0.002765 0.002753 0.002652 0.000738 0.001001 0.108135 
I0504 02:16:12.704099 15008 sgd_solver.cpp:106] Iteration 0, lr = 0.001
I0504 02:16:25.151051 15008 solver.cpp:478] Snapshotting to binary proto file ../caffe_model/snapshot/model_transfer_iter_50.caffemodel
I0504 02:16:26.370049 15008 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ../caffe_model/snapshot/model_transfer_iter_50.solverstate
I0504 02:16:26.782049 15008 solver.cpp:234] Iteration 50, loss = 0.183239
I0504 02:16:26.782587 15008 solver.cpp:250]     Train net output #0: loss = 0.183239 (* 1 = 0.183239 loss)
I0504 02:16:26.963050 15008 solver.cpp:264] layer blob norm:0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000001 0.000357 
I0504 02:16:26.965549 15008 solver.cpp:272] weight blob norm:0.005708 0.003213 0.001602 0.001632 0.001414 0.000196 0.000177 0.021635 
I0504 02:16:26.965549 15008 sgd_solver.cpp:106] Iteration 50, lr = 0.001
I0504 02:16:39.477463 15008 solver.cpp:478] Snapshotting to binary proto file ../caffe_model/snapshot/model_transfer_iter_100.caffemodel
I0504 02:16:40.653507 15008 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ../caffe_model/snapshot/model_transfer_iter_100.solverstate
I0504 02:16:40.955008 15008 solver.cpp:361] Iteration 100, Testing net (#0)
I0504 02:17:01.285140 15008 solver.cpp:428]     Test net output #0: accuracy = 0.96732
I0504 02:17:01.285640 15008 solver.cpp:428]     Test net output #1: loss = 0.0830689 (* 1 = 0.0830689 loss)
I0504 02:17:01.384639 15008 solver.cpp:234] Iteration 100, loss = 0.127209
I0504 02:17:01.385639 15008 solver.cpp:250]     Train net output #0: loss = 0.127209 (* 1 = 0.127209 loss)
I0504 02:17:01.551141 15008 solver.cpp:264] layer blob norm:0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000001 0.000210 
I0504 02:17:01.554139 15008 solver.cpp:272] weight blob norm:0.004899 0.001856 0.000970 0.000963 0.000821 0.000117 0.000102 0.010729 
I0504 02:17:01.554641 15008 sgd_solver.cpp:106] Iteration 100, lr = 0.001
I0504 02:17:14.106267 15008 solver.cpp:478] Snapshotting to binary proto file ../caffe_model/snapshot/model_transfer_iter_150.caffemodel
I0504 02:17:15.252352 15008 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ../caffe_model/snapshot/model_transfer_iter_150.solverstate
I0504 02:17:15.665874 15008 solver.cpp:234] Iteration 150, loss = 0.0624567
I0504 02:17:15.666405 15008 solver.cpp:250]     Train net output #0: loss = 0.0624567 (* 1 = 0.0624567 loss)
I0504 02:17:15.846371 15008 solver.cpp:264] layer blob norm:0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000001 0.000164 
I0504 02:17:15.848871 15008 solver.cpp:272] weight blob norm:0.003634 0.001257 0.000617 0.000645 0.000493 0.000076 0.000067 0.006335 
I0504 02:17:15.848871 15008 sgd_solver.cpp:106] Iteration 150, lr = 0.001
I0504 02:17:28.798450 15008 solver.cpp:478] Snapshotting to binary proto file ../caffe_model/snapshot/model_transfer_iter_200.caffemodel
I0504 02:17:29.956444 15008 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ../caffe_model/snapshot/model_transfer_iter_200.solverstate
I0504 02:17:30.254943 15008 solver.cpp:361] Iteration 200, Testing net (#0)
I0504 02:17:50.647167 15008 solver.cpp:428]     Test net output #0: accuracy = 0.976119
I0504 02:17:50.647668 15008 solver.cpp:428]     Test net output #1: loss = 0.0607265 (* 1 = 0.0607265 loss)
I0504 02:17:50.747666 15008 solver.cpp:234] Iteration 200, loss = 0.0804356
I0504 02:17:50.748195 15008 solver.cpp:250]     Train net output #0: loss = 0.0804356 (* 1 = 0.0804356 loss)
I0504 02:17:50.912667 15008 solver.cpp:264] layer blob norm:0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000001 0.000181 
I0504 02:17:50.915166 15008 solver.cpp:272] weight blob norm:0.003606 0.001743 0.000865 0.000840 0.000723 0.000093 0.000080 0.009261 
I0504 02:17:50.915166 15008 sgd_solver.cpp:106] Iteration 200, lr = 0.001
I0504 02:18:03.585413 15008 solver.cpp:478] Snapshotting to binary proto file ../caffe_model/snapshot/model_transfer_iter_250.caffemodel
I0504 02:18:04.737928 15008 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ../caffe_model/snapshot/model_transfer_iter_250.solverstate
I0504 02:18:05.145447 15008 solver.cpp:234] Iteration 250, loss = 0.0587572
I0504 02:18:05.146448 15008 solver.cpp:250]     Train net output #0: loss = 0.0587572 (* 1 = 0.0587572 loss)
I0504 02:18:05.325947 15008 solver.cpp:264] layer blob norm:0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000001 0.000177 
I0504 02:18:05.328948 15008 solver.cpp:272] weight blob norm:0.003271 0.001168 0.000566 0.000553 0.000496 0.000084 0.000074 0.006980 
I0504 02:18:05.328948 15008 sgd_solver.cpp:106] Iteration 250, lr = 0.001
I0504 02:18:17.972028 15008 solver.cpp:478] Snapshotting to binary proto file ../caffe_model/snapshot/model_transfer_iter_300.caffemodel
I0504 02:18:19.131031 15008 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ../caffe_model/snapshot/model_transfer_iter_300.solverstate
I0504 02:18:19.437535 15008 solver.cpp:361] Iteration 300, Testing net (#0)
I0504 02:18:39.875859 15008 solver.cpp:428]     Test net output #0: accuracy = 0.980879
I0504 02:18:39.876359 15008 solver.cpp:428]     Test net output #1: loss = 0.0494896 (* 1 = 0.0494896 loss)
I0504 02:18:39.975857 15008 solver.cpp:234] Iteration 300, loss = 0.0864382
I0504 02:18:39.976358 15008 solver.cpp:250]     Train net output #0: loss = 0.0864382 (* 1 = 0.0864382 loss)
I0504 02:18:40.140856 15008 solver.cpp:264] layer blob norm:0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000134 
I0504 02:18:40.144382 15008 solver.cpp:272] weight blob norm:0.003571 0.001337 0.000640 0.000633 0.000520 0.000072 0.000059 0.006090 
I0504 02:18:40.144878 15008 sgd_solver.cpp:106] Iteration 300, lr = 0.001
I0504 02:18:44.746919 15008 solver.cpp:478] Snapshotting to binary proto file ../caffe_model/snapshot/model_transfer_iter_319.caffemodel
